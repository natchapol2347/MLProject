{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deca803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3e5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iam_dataset(data_path, img_height, img_width):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(os.path.join(data_path, 'words.txt')) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.strip().split(' ')\n",
    "            part1 = parts[0].split('-')\n",
    "            file_path = os.path.join(data_path, part1[0])\n",
    "            file_path = os.path.join(file_path, part1[0]+'-'+part1[1])\n",
    "            file_path = os.path.join(file_path, parts[0] + '.png')\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "            # print(file_path)\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # print(img_width,img_height)\n",
    "            try:\n",
    "                img = cv2.resize(img, (img_width, img_height))\n",
    "                images.append(img)\n",
    "                labels.append(parts[-1])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    images = np.array(images).reshape(-1, img_height, img_width, 1).astype('float32') / 255\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8acb86e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iam_data_path = \"words\"\n",
    "\n",
    "# Set your desired image height and width\n",
    "img_height = 64\n",
    "img_width = 128\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "images, labels = load_iam_dataset(iam_data_path, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d08b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf6c805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fab5c0a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(input_shape, num_classes)\n\u001b[0;32m     29\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "def create_model(input_shape, num_classes):\n",
    "    input_data = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # CNN layers\n",
    "    cnn = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_data)\n",
    "    cnn = layers.MaxPooling2D((2, 2))(cnn)\n",
    "    cnn = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(cnn)\n",
    "    cnn = layers.MaxPooling2D((2, 2))(cnn)\n",
    "    \n",
    "    # Prepare output for RNN\n",
    "    shape = cnn.get_shape().as_list()\n",
    "    rnn_input = layers.Reshape(target_shape=(shape[1], shape[2] * shape[3]))(cnn)\n",
    "    \n",
    "    # RNN layers\n",
    "    rnn = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.25))(rnn_input)\n",
    "    rnn = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.25))(rnn)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(num_classes + 1, activation='softmax', name='output')(rnn)\n",
    "\n",
    "    model = models.Model(inputs=input_data, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (img_height, img_width, 1)\n",
    "model = create_model(input_shape, num_classes)\n",
    "num_classes = np.max(y_train) + 1\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5b43e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    # Calculate label and logit lengths\n",
    "    print(\"y_true shape:\", y_true.shape)\n",
    "    print(\"y_pred shape:\", y_pred.shape)\n",
    "\n",
    "    label_length = tf.reduce_sum(tf.cast(tf.math.not_equal(y_true, -1), tf.int32), axis=-1)\n",
    "    logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])\n",
    "\n",
    "    return tf.nn.ctc_loss(\n",
    "        labels=y_true,\n",
    "        logits=y_pred,\n",
    "        label_length=label_length,\n",
    "        logit_length=logit_length,\n",
    "        logits_time_major=False,\n",
    "        blank_index=-1\n",
    "    )\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x_batch, training=True)\n",
    "        loss_value = ctc_loss(y_batch, logits)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "827a2397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2901\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_train):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i,sequence)\n\u001b[1;32m---> 11\u001b[0m     y_train_padded[i, :\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m sequence\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare the dataset for training\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((x_train, y_train_padded))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "optimizer = Adam()\n",
    "\n",
    "# Assuming y_train is a NumPy array of shape (num_samples,)\n",
    "# max_label_length = np.max([len(sequence) for sequence in y_train])\n",
    "y_train_padded = np.full((y_train.shape[0], len(y_train)), -1)\n",
    "\n",
    "for i, sequence in enumerate(y_train):\n",
    "    print(i,sequence)\n",
    "    y_train_padded[i, :len(sequence)] = sequence\n",
    "\n",
    "\n",
    "# Prepare the dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_padded)).batch(batch_size)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        print(x_batch.shape)\n",
    "        print(y_batch.shape)\n",
    "        loss_value = train_step(x_batch, y_batch)\n",
    "        print(f\"Step {step + 1}: loss = {loss_value.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_handwriting_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ab105e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      2\u001b[0m iam_model_pred \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m----> 3\u001b[0m \u001b[43miam_model_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/my_handwriting_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2644\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2640\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2641\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`load_weights` requires h5py package when loading weights from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2642\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHDF5. Try installing h5py.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 2644\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2645\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load weights saved in HDF5 format into a subclassed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2646\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel which has not created its variables yet. Call the Model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2647\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst, then load the weights.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_weights_created()\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "iam_model_pred = Model()\n",
    "iam_model_pred.load_weights(filepath='/my_handwriting_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_predictions = np.sum(y_test == predicted_words)\n",
    "total_predictions = len(y_test)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e893131",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.imshow(x_test[i].reshape(img_height, img_width), cmap=\"gray\")\n",
    "    plt.title(f\"True: {y_test[i]}\\nPredicted: {predicted_words[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0fd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
